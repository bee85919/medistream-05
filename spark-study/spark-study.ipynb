{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"medistream-05\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get id column in data (feat.alias)\n",
    "from pyspark.sql.functions import col\n",
    "df = data.select(\n",
    "    col(\"id\").alias(\"id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### regexp_replace\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "df = data.withColumn(\n",
    "    \"description\",\n",
    "    regexp_replace(\"description\", \"[\\n\\r*,]\", \"\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column's length\n",
    "from pyspark.sql.functions import length\n",
    "df = data.withColumn(\n",
    "    \"description_length\",\n",
    "    length(\"description\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count column's size\n",
    "from pyspark.sql.functions import size\n",
    "df = data.withColumn(\n",
    "    \"images_count\", \n",
    "    size(\"images\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string + list => list\n",
    "from pyspark.sql.functions import flatten, array\n",
    "df = data.withColumn(\n",
    "    # get homepages' urls\n",
    "    'homepages_url', \n",
    "    flatten(array(array('homepages_repr.url'), 'homepages_etc.url'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startswith\n",
    "from pyspark.sql.functions import startswith\n",
    "df = data.withColumn(\n",
    "    # get homepages' urls\n",
    "    'is_smart_phone',\n",
    "    col('phone').startswith('010')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array contains\n",
    "from pyspark.sql.functions import array_contains\n",
    "df = data.withColumn(\n",
    "    'is_zero_pay',\n",
    "    array_contains(col('payment_info'), '제로페이')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list[0] & -> col\n",
    "from pyspark.sql.functions import array_contains\n",
    "df = data.withColumn(\n",
    "    'keywords_1',\n",
    "    col('keywords')[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get list\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import concat_ws\n",
    "# get array type columns\n",
    "arr_col_lst = [field.name for field in df.schema.fields if isinstance(field.dataType, ArrayType)]\n",
    "# concat_ws to array type columns\n",
    "for arr_col in arr_col_lst:\n",
    "    df = df.withColumn(arr_col, concat_ws(\",\", arr_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left outer join df & df2\n",
    "df = df.join(df2, df.id == df.root_id, \"left_outer\") # id를 비교\n",
    "df = df.drop(\"root_id\") # 불필요해진 root_id drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to redshift\n",
    "# set vars\n",
    "jdbc_url = jdbc_url\n",
    "temp_dir = temp_dir\n",
    "db_table = db_table\n",
    "# df를 redshift에 적재한다.\n",
    "df.write \\\n",
    "  .format(\"io.github.spark_redshift_community.spark.redshift\") \\  # df.write의 format 설정\n",
    "  .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\        # df.write의 driver 설정\n",
    "  .option(\"forward_spark_s3_credentials\", True) \\                 # df.write의 forward_spark_s3_credentials 설정 # IAM_ROLE이 있다면 IAM_ROLE을 사용\n",
    "  .option(\"url\", jdbc_url) \\\n",
    "  .option(\"dbtable\", db_table) \\\n",
    "  .option(\"tempdir\", temp_dir) \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
