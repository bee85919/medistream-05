{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T12:44:54.358050Z",
     "iopub.status.busy": "2023-12-12T12:44:54.357722Z",
     "iopub.status.idle": "2023-12-12T12:45:28.941012Z",
     "shell.execute_reply": "2023-12-12T12:45:28.940409Z",
     "shell.execute_reply.started": "2023-12-12T12:44:54.358014Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### import pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import array, array_contains, col, concat_ws, explode, flatten, length, max, regexp_extract, regexp_replace, size, substring, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T12:45:28.942307Z",
     "iopub.status.busy": "2023-12-12T12:45:28.942070Z",
     "iopub.status.idle": "2023-12-12T12:45:28.989904Z",
     "shell.execute_reply": "2023-12-12T12:45:28.989310Z",
     "shell.execute_reply.started": "2023-12-12T12:45:28.942274Z"
    }
   },
   "outputs": [],
   "source": [
    "### create spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"medistream-05\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 경로 값을 할당한다.\n",
    "json_path = json_path\n",
    "save_path = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T12:45:28.991256Z",
     "iopub.status.busy": "2023-12-12T12:45:28.991012Z",
     "iopub.status.idle": "2023-12-12T12:45:29.037151Z",
     "shell.execute_reply": "2023-12-12T12:45:29.036604Z",
     "shell.execute_reply.started": "2023-12-12T12:45:28.991225Z"
    }
   },
   "outputs": [],
   "source": [
    "### get start time\n",
    "st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T12:45:29.038486Z",
     "iopub.status.busy": "2023-12-12T12:45:29.038246Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### read json data\n",
    "json_path = json_path\n",
    "data = spark.read.json(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. hospital key의 json object 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hospital key의 json object를 hospital_data의 변수에 할당했다. alias(\"h\") 옵션을 주어 이름을 \"h\"로 변경했다.\n",
    "hospital_data = data.select(explode(\"hospital\").alias(\"h\"))\n",
    "\n",
    "### hospital_data에서 필요한 item을 가져와서 hospital_df의 변수에 할당했다.\n",
    "hospital_df = hospital_data.select(\n",
    "    col(\"h.id\").alias(\"id\"),\n",
    "    col(\"h.name\").alias(\"name\"),\n",
    "    col(\"h.category\").alias(\"category\"),\n",
    "    col(\"h.category_code\").alias(\"category_code\"),\n",
    "    col(\"h.category_code_list\").alias(\"category_code_list\"),\n",
    "    col(\"h.category_count\").alias(\"category_count\"),\n",
    "    col(\"h.description\").alias(\"description\"),\n",
    "    col(\"h.road_address\").alias(\"road_address\"),\n",
    "    col(\"h.road\").alias(\"road\"),\n",
    "    col(\"h.rcode\").alias(\"rcode\"),\n",
    "    col(\"h.virtual_phone\").alias(\"virtual_phone\"),\n",
    "    col(\"h.phone\").alias(\"phone\"),\n",
    "    col(\"h.payment_info\").alias(\"payment_info\"),\n",
    "    col(\"h.conveniences\").alias(\"conveniences\"),\n",
    "    col(\"h.review_setting.keyword\").alias(\"review_keyword\"),\n",
    "    col(\"h.keywords\").alias(\"keywords\"),\n",
    "    col(\"h.booking_business_id\").alias(\"booking_business_id\"),\n",
    "    col(\"h.booking_display_name\").alias(\"booking_display_name\"),\n",
    "    col(\"h.visitor_reviews_score\").alias(\"visitor_reviews_score\"),\n",
    "    col(\"h.visitor_reviews_total\").alias(\"visitor_reviews_total\"),\n",
    "    col(\"h.visitor_reviews_text_review_total\").alias(\"visitor_reviews_text_review_total\"),\n",
    "    col(\"h.images\").alias(\"images\"),\n",
    "    col(\"h.homepages.etc\").alias(\"homepages_etc\"),\n",
    "    col(\"h.homepages.repr\").alias(\"homepages_repr\"),\n",
    "    col(\"h.homepages.repr.url\").alias(\"is_rep\"), # isRep?\n",
    "    col(\"h.booking_url\").alias(\"booking_url\"),\n",
    "    col(\"h.talktalk_url\").alias(\"talktalk_url\"),\n",
    "    col(\"h.coordinate.x\").alias(\"lon\"),\n",
    "    col(\"h.coordinate.y\").alias(\"lat\"),\n",
    ")\n",
    "\n",
    "\n",
    "### hospital_df의 전처리 과정을 진행했다. \n",
    "\n",
    "# 문자열 변경\n",
    "hospital_df = hospital_df.withColumn(\n",
    "    \"description\",\n",
    "    regexp_replace(\"description\", \"[\\n\\r*,]\", \"\") \n",
    ").withColumn(\n",
    "    \"road\",\n",
    "    regexp_replace(\"road\", \"[\\n\\r*,]\", \"\")\n",
    ").withColumn(\n",
    "    \"review_keyword\",\n",
    "    regexp_replace(\"review_keyword\", \"[\\\\\\\"]\", \"\")\n",
    ")\n",
    "\n",
    "# 기타 전처리\n",
    "hospital_df = hospital_df.withColumn(\n",
    "    # get description's length\n",
    "    \"description_length\",\n",
    "    length(\"description\")\n",
    ").withColumn(\n",
    "    # count images\n",
    "    \"images_count\", \n",
    "    size(\"images\")\n",
    ").withColumn(\n",
    "    # get photo_review_ratio\n",
    "    'photo_review_ratio',\n",
    "    (col('visitor_reviews_total')-col('visitor_reviews_text_review_total'))/col('visitor_reviews_total')\n",
    ").withColumn(\n",
    "    # get homepages' urls\n",
    "    'homepages_url', \n",
    "    flatten(array(array('homepages_repr.url'), 'homepages_etc.url'))\n",
    ").withColumn(\n",
    "    # get homepages' types\n",
    "    'homepages_type', \n",
    "    flatten(array(array('homepages_repr.type'), 'homepages_etc.type'))\n",
    ").withColumn(\n",
    "    # get homepages' order\n",
    "    'homepages_order', \n",
    "    when(\n",
    "        col('homepages_repr.order').isNull(), 0\n",
    "    ).otherwise(\n",
    "        size(flatten(array(array('homepages_repr.order'), 'homepages_etc.order')))\n",
    "    )\n",
    ").withColumn(\n",
    "    # get boolean of smart phone\n",
    "    'is_smart_phone',\n",
    "    col('phone').startswith('010')\n",
    ").withColumn(\n",
    "    # get boolean of zero pay\n",
    "    'is_zero_pay',\n",
    "    array_contains(col('payment_info'), '제로페이')\n",
    ").withColumn(\n",
    "    # get boolean of dead url\n",
    "    'is_dead_url',\n",
    "    flatten(array(array('homepages_repr.isDeadUrl'), 'homepages_etc.isDeadUrl'))\n",
    ").withColumn(\n",
    "    # get 1st keyword\n",
    "    'keywords_1',\n",
    "    col('keywords')[0]\n",
    ").withColumn(\n",
    "    # get 2nd keyword\n",
    "    'keywords_2',\n",
    "    col('keywords')[1]\n",
    ").withColumn(\n",
    "    # get 3rd keyword\n",
    "    'keywords_3',\n",
    "    col('keywords')[2]\n",
    ").withColumn(\n",
    "    # get 4th keyword\n",
    "    'keywords_4',\n",
    "    col('keywords')[3]\n",
    ").withColumn(\n",
    "    # get 5th keyword\n",
    "    'keywords_5',\n",
    "    col('keywords')[4]\n",
    ")\n",
    "\n",
    "# drop unnecessary columns\n",
    "hospital_df = hospital_df.drop(\n",
    "    \"images\", \n",
    "    \"keywords\", \n",
    "    \"homepages_repr\", \n",
    "    \"homepages_etc\"\n",
    ").withColumn(\n",
    "    # type casting\n",
    "    \"description\",\n",
    "    col(\"description\").cast(StringType())\n",
    ").withColumn(\n",
    "    # type casting\n",
    "    \"road\",\n",
    "    col(\"road\").cast(StringType())\n",
    ").withColumn(\n",
    "    # type casting\n",
    "    \"road_address\",\n",
    "    col(\"road_address\").cast(StringType())   \n",
    ").withColumn(\n",
    "    # type casting\n",
    "    \"is_smart_phone\", \n",
    "    col(\"is_smart_phone\").cast(StringType())\n",
    ").withColumn(\n",
    "    # type casting\n",
    "    \"is_zero_pay\", \n",
    "    col(\"is_zero_pay\").cast(StringType())\n",
    ")\n",
    "\n",
    "\n",
    "### 배열 삽입을 위한 전처리 과정\n",
    "\n",
    "# get array type columns\n",
    "arr_col_lst = [field.name for field in hospital_df.schema.fields if isinstance(field.dataType, ArrayType)]\n",
    "\n",
    "# concat_ws to array type columns\n",
    "for arr_col in arr_col_lst:\n",
    "    hospital_df = hospital_df.withColumn(arr_col, concat_ws(\",\", arr_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. root key의 json object 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### root key의 json object를 불러온다. root key의 alias는 r, 변수명은 root_data이다.\n",
    "root_data = data.select(explode(\"root\").alias(\"r\"))\n",
    "\n",
    "### root key의 json object에서 필요한 item을 가져와서 root_df에 할당했다.\n",
    "root_df = root_data.select(\n",
    "    col(\"r.hospital.base.__ref\").alias(\"root_id\"),\n",
    "    col(\"r.hospital.fsasReviews.total\").alias(\"fsas_reviews_count\"),\n",
    "    col(\"r.hospital.kinQna.answerCount\").alias(\"kin_qna_count\")\n",
    ")\n",
    "\n",
    "### root_df의 문자열 전처리를 진행했다.\n",
    "root_df = root_df.withColumn(\n",
    "    \"root_id\",\n",
    "    regexp_extract(\"root_id\", \"HospitalBase:([\\\\w]+)\", 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hospital_df와 root_df를 left outer join해서 df로 만들었다.\n",
    "\n",
    "# 올바른 값이 가져오기 위해 id를 비교한다.\n",
    "df = hospital_df.join(root_df, hospital_df.id == root_df.root_id, \"left_outer\")\n",
    "\n",
    "# 불필요해진 root_id 값은 drop한다.\n",
    "df = df.drop(\"root_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. save merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### df를 저장한다.\n",
    "\n",
    "# 중복열을 제거한다.\n",
    "df.dropDuplicates()\n",
    "\n",
    "# parquet type으로 df를 저장한다. option은 overwrite이다.\n",
    "df.write.mode('overwrite').parquet(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. check task time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 시간을 측정한다.\n",
    "ft = time.time()\n",
    "print(f\"pyspark task time: {ft - st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. upload to redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 필요한 변수들을 할당한다.\n",
    "jdbc_url = jdbc_url\n",
    "temp_dir = temp_dir\n",
    "db_table = db_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### df를 redshift에 적재한다.\n",
    "df.write \\\n",
    "  .format(\"io.github.spark_redshift_community.spark.redshift\") \\  # df.write의 format 설정\n",
    "  .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\        # df.write의 driver 설정\n",
    "  .option(\"forward_spark_s3_credentials\", True) \\                 # df.write의 forward_spark_s3_credentials 설정\n",
    "  .option(\"url\", jdbc_url) \\\n",
    "  .option(\"dbtable\", db_table) \\\n",
    "  .option(\"tempdir\", temp_dir) \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark의 사용을 종료한다.\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
